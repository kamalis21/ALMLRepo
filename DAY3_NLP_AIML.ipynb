{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d399c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6835543766578249\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.46      0.52       151\n",
      "           1       0.58      0.63      0.61       202\n",
      "           2       0.61      0.63      0.62       195\n",
      "           3       0.54      0.69      0.61       183\n",
      "           4       0.76      0.62      0.69       205\n",
      "           5       0.81      0.79      0.80       215\n",
      "           6       0.75      0.69      0.72       193\n",
      "           7       0.70      0.68      0.69       196\n",
      "           8       0.42      0.73      0.53       168\n",
      "           9       0.83      0.79      0.81       211\n",
      "          10       0.90      0.87      0.88       198\n",
      "          11       0.77      0.75      0.76       201\n",
      "          12       0.73      0.57      0.64       202\n",
      "          13       0.81      0.80      0.81       194\n",
      "          14       0.76      0.75      0.76       189\n",
      "          15       0.53      0.86      0.65       202\n",
      "          16       0.67      0.76      0.71       188\n",
      "          17       0.79      0.74      0.76       182\n",
      "          18       0.75      0.50      0.60       159\n",
      "          19       0.64      0.05      0.10       136\n",
      "\n",
      "    accuracy                           0.68      3770\n",
      "   macro avg       0.70      0.67      0.66      3770\n",
      "weighted avg       0.70      0.68      0.68      3770\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 70   0   3   0   0   1   0   1   6   2   1   2   0   4   4  34   7   9\n",
      "    4   3]\n",
      " [  1 128  17  14   2  11   3   2  10   0   0   2   2   2   4   3   0   1\n",
      "    0   0]\n",
      " [  1  17 123  23   5  10   0   1   9   1   0   3   1   0   0   1   0   0\n",
      "    0   0]\n",
      " [  0  10  19 127  12   3   4   1   1   0   0   0   3   2   1   0   0   0\n",
      "    0   0]\n",
      " [  0   6  10  24 128   1   6   2  12   0   0   4   5   1   3   2   1   0\n",
      "    0   0]\n",
      " [  0  21  12   2   1 169   2   1   3   0   0   0   0   1   1   2   0   0\n",
      "    0   0]\n",
      " [  0   4   2  21   7   1 134   6   3   1   0   3   4   0   5   1   1   0\n",
      "    0   0]\n",
      " [  1   2   0   3   0   4   4 133  24   0   1   0   5   3   5   3   5   1\n",
      "    2   0]\n",
      " [  0   3   0   1   0   0   6  13 123   6   1   1   3   2   3   2   3   0\n",
      "    1   0]\n",
      " [  1   1   0   0   0   0   2   1  16 167   8   2   0   2   2   6   0   3\n",
      "    0   0]\n",
      " [  1   0   0   0   0   1   0   3   9   4 173   2   1   2   0   1   0   1\n",
      "    0   0]\n",
      " [  1   4   3   0   1   3   1   1   4   1   1 150   7   2   1   4   8   4\n",
      "    5   0]\n",
      " [  1  10   5  16  11   0  14   8   8   2   1   3 116   3   2   2   0   0\n",
      "    0   0]\n",
      " [  1   6   1   1   1   0   0   2   7   2   2   2   6 156   2   2   2   0\n",
      "    1   0]\n",
      " [  1   6   0   3   0   0   2   2  13   2   1   5   4   4 142   2   1   0\n",
      "    1   0]\n",
      " [  3   0   2   0   0   2   0   0   6   1   0   1   0   1   0 174   2   7\n",
      "    2   1]\n",
      " [  1   0   1   0   0   0   0   2  15   3   2   7   0   0   3   6 143   0\n",
      "    5   0]\n",
      " [  7   1   0   0   0   2   1   2   9   4   1   3   1   2   1  10   2 134\n",
      "    2   0]\n",
      " [  2   1   1   1   0   0   0   7   6   1   1   3   1   4   4  13  28   6\n",
      "   80   0]\n",
      " [ 27   0   1   0   0   1   0   1   8   3   0   3   0   2   3  63  11   3\n",
      "    3   7]]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Load the 20 Newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert text data to feature vectors using TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = nb_classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4746f4bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': '-DOCSTART-', 'pos': '-X-', 'chunk': '-X-', 'label': 'O'}]\n",
      "\n",
      "[{'word': 'EU', 'pos': 'NNP', 'chunk': 'B-NP', 'label': 'B-ORG'}, {'word': 'rejects', 'pos': 'VBZ', 'chunk': 'B-VP', 'label': 'O'}, {'word': 'German', 'pos': 'JJ', 'chunk': 'B-NP', 'label': 'B-MISC'}, {'word': 'call', 'pos': 'NN', 'chunk': 'I-NP', 'label': 'O'}, {'word': 'to', 'pos': 'TO', 'chunk': 'B-VP', 'label': 'O'}, {'word': 'boycott', 'pos': 'VB', 'chunk': 'I-VP', 'label': 'O'}, {'word': 'British', 'pos': 'JJ', 'chunk': 'B-NP', 'label': 'B-MISC'}, {'word': 'lamb', 'pos': 'NN', 'chunk': 'I-NP', 'label': 'O'}, {'word': '.', 'pos': '.', 'chunk': 'O', 'label': 'O'}]\n",
      "\n",
      "[{'word': 'Peter', 'pos': 'NNP', 'chunk': 'B-NP', 'label': 'B-PER'}, {'word': 'Blackburn', 'pos': 'NNP', 'chunk': 'I-NP', 'label': 'I-PER'}]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to read CoNLL 2003 dataset\n",
    "def read_conll_data(file_path):\n",
    "    sentences = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        sentence = []\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    sentence = []\n",
    "            else:\n",
    "                tokens = line.split()\n",
    "                word, pos, chunk, label = tokens[0], tokens[1], tokens[2], tokens[3]\n",
    "                sentence.append({'word': word, 'pos': pos, 'chunk': chunk, 'label': label})\n",
    "    return sentences\n",
    "\n",
    "# Example usage\n",
    "conll_file_path = \"C:\\\\Users\\\\hp\\\\Documents\\\\train.txt\"  # Replace with the actual path to your CoNLL 2003 dataset file\n",
    "dataset = read_conll_data(conll_file_path)\n",
    "\n",
    "# Displaying the first few sentences\n",
    "for i in range(3):\n",
    "    print(dataset[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd06fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
