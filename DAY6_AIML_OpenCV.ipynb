{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b86c6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the Time from where you want to trim1\n",
      "Enter the Time till where you want to trim2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "vid=cv2.VideoCapture(\"C:\\\\Users\\\\hp\\\\output_video.mp4\")\n",
    "count = 0\n",
    "start=int(input(\"Enter the Time from where you want to trim\"))\n",
    "finall=int(input(\"Enter the Time till where you want to trim\"))\n",
    "vid2=cv2.VideoWriter(\"NewVideo.mp4\",0,1,(480,480))\n",
    "while vid.isOpened():\n",
    "    ret, frame = vid.read()\n",
    "    if not ret:break\n",
    "    if count >=start and count< finall:\n",
    "        vid2.write(frame)\n",
    "    count += 1\n",
    "vid.release()\n",
    "vid2.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eaebddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging completed. Output video saved to C:\\Users\\hp\\output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def merge_videos(video1_path, video2_path, output_path):\n",
    "    # Open the first video file\n",
    "    video1 = cv2.VideoCapture(video1_path)\n",
    "    if not video1.isOpened():\n",
    "        print(f\"Error: Could not open video file {video1_path}\")\n",
    "        return\n",
    "\n",
    "    # Open the second video file\n",
    "    video2 = cv2.VideoCapture(video2_path)\n",
    "    if not video2.isOpened():\n",
    "        print(f\"Error: Could not open video file {video2_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(video1.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(video1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to write the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # You can change the codec as needed\n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, fps, (width * 2, height))\n",
    "\n",
    "    while True:\n",
    "        # Read frames from both videos\n",
    "        ret1, frame1 = video1.read()\n",
    "        ret2, frame2 = video2.read()\n",
    "\n",
    "        # Break the loop if either video has no more frames\n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "\n",
    "        # Resize frames to have the same height\n",
    "        frame1 = cv2.resize(frame1, (width, height))\n",
    "        frame2 = cv2.resize(frame2, (width, height))\n",
    "\n",
    "        # Concatenate the frames horizontally\n",
    "        merged_frame = np.concatenate((frame1, frame2), axis=1)\n",
    "\n",
    "        # Write the merged frame to the output video\n",
    "        output_video.write(merged_frame)\n",
    "\n",
    "    # Release resources\n",
    "    video1.release()\n",
    "    video2.release()\n",
    "    output_video.release()\n",
    "\n",
    "    print(f\"Merging completed. Output video saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video1_path = \"C:\\\\Users\\\\hp\\\\newRunning.mp4\"\n",
    "    video2_path = \"C:\\\\Users\\\\hp\\\\output_video.mp4\"\n",
    "    output_path = \"C:\\\\Users\\\\hp\\\\output_video.mp4\"\n",
    "\n",
    "    merge_videos(video1_path, video2_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "005e1bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-second video creation completed. Output video saved to C:\\Users\\hp\\output_video.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def merge_and_create_5sec_video(video1_path, video2_path, output_path):\n",
    "    video1 = cv2.VideoCapture(video1_path)\n",
    "    if not video1.isOpened():\n",
    "        print(f\"Error: Could not open video file {video1_path}\")\n",
    "        return\n",
    "    video2 = cv2.VideoCapture(video2_path)\n",
    "    if not video2.isOpened():\n",
    "        print(f\"Error: Could not open video file {video2_path}\")\n",
    "        return\n",
    "    fps = int(video1.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(video1.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video1.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "   \n",
    "    duration_sec = 5\n",
    "    num_frames = int(fps * duration_sec)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "    output_video = cv2.VideoWriter(output_path, fourcc, fps, (width * 2, height))\n",
    "\n",
    "    \n",
    "        ret1, frame1 = video1.read()\n",
    "        ret2, frame2 = video2.read()\n",
    "\n",
    "      \n",
    "        if not ret1 or not ret2:\n",
    "            break\n",
    "\n",
    "        frame1 = cv2.resize(frame1, (width, height))\n",
    "        frame2 = cv2.resize(frame2, (width, height))\n",
    "\n",
    "       \n",
    "        merged_frame = np.concatenate((frame1, frame2), axis=1)\n",
    "\n",
    "        output_video.write(merged_frame)\n",
    "\n",
    "    video1.release()\n",
    "    video2.release()\n",
    "    output_video.release()\n",
    "\n",
    "    print(f\"5-second video creation completed. Output video saved to {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video1_path = \"C:\\\\Users\\\\hp\\\\newRunning.mp4\"\n",
    "    video2_path = \"C:\\\\Users\\\\hp\\\\output_video.mp4\"\n",
    "    output_path = \"C:\\\\Users\\\\hp\\\\output_video.mp4\"\n",
    "\n",
    "    merge_and_create_5sec_video(video1_path, video2_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ce54dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canny edge detection completed. Output video saved to output_canny_edges.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def apply_canny_edge_detection(input_video_path, output_video_path, lower_threshold=50, upper_threshold=150):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(input_video_path)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to write the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Change the codec as needed\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=False)\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        # Break the loop if there are no more frames\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply Canny edge detection\n",
    "        edges = cv2.Canny(gray_frame, lower_threshold, upper_threshold)\n",
    "\n",
    "        # Convert the edges back to BGR\n",
    "        edges_bgr = cv2.cvtColor(edges, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        output_video.write(edges_bgr)\n",
    "\n",
    "        # Display the original and processed frames (optional)\n",
    "        cv2.imshow('Original Frame', frame)\n",
    "        cv2.imshow('Canny Edges', edges_bgr)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    video.release()\n",
    "    output_video.release()\n",
    "    cv2.destroyAllWindows()  # Ensure all OpenCV windows are closed\n",
    "\n",
    "    print(f\"Canny edge detection completed. Output video saved to {output_video_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_video_path = \"C:\\\\Users\\\\hp\\\\newRunning.mp4\"\n",
    "    output_video_path = \"output_canny_edges.mp4\"\n",
    "\n",
    "    apply_canny_edge_detection(input_video_path, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12300de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semicircle detection completed. Output video saved to output_semicircle_detection.avi\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#circle\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def detect_semicircles(input_video_path, output_video_path):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(input_video_path)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Create VideoWriter object to write the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Change the codec as needed\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height), isColor=True)\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Apply GaussianBlur to reduce noise and help circle detection\n",
    "        blurred = cv2.GaussianBlur(gray, (5, 5), 0)\n",
    "\n",
    "        # Use Hough Circle Transform to detect circles\n",
    "        circles = cv2.HoughCircles(\n",
    "            blurred,\n",
    "            cv2.HOUGH_GRADIENT,\n",
    "            dp=1,\n",
    "            minDist=50,\n",
    "            param1=50,\n",
    "            param2=30,\n",
    "            minRadius=30,\n",
    "            maxRadius=80\n",
    "        )\n",
    "\n",
    "        # If circles are found, draw them on the frame\n",
    "        if circles is not None:\n",
    "            circles = np.uint16(np.around(circles))\n",
    "            for circle in circles[0, :]:\n",
    "                center = (circle[0], circle[1])\n",
    "                radius = circle[2]\n",
    "                \n",
    "                # Check if the detected circle is a semicircle (half circle)\n",
    "                if center[1] + radius < height:\n",
    "                    cv2.circle(frame, center, radius, (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        output_video.write(frame)\n",
    "\n",
    "        # Display the result (optional)\n",
    "        cv2.imshow('Semicircle Detection', frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    video.release()\n",
    "    output_video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"Semicircle detection completed. Output video saved to {output_video_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_video_path = \"C:\\\\Users\\\\hp\\\\newRunning.mp4\"\n",
    "    output_video_path = \"output_semicircle_detection.avi\"\n",
    "\n",
    "    detect_semicircles(input_video_path, output_video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ae2d058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object tracking completed. Output video saved to C:\\Users\\hp\\output_histogram_tracking.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Histogram \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def histogram_based_tracking(input_video_path, output_video_path, roi_coords):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(input_video_path)\n",
    "    if not video.isOpened():\n",
    "        print(f\"Error: Could not open video file {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties\n",
    "    fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Set the region of interest (ROI) for tracking\n",
    "    x, y, w, h = roi_coords\n",
    "    roi = (x, y, w, h)\n",
    "\n",
    "    # Create VideoWriter object to write the output\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Change the codec as needed\n",
    "    output_video = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    # Read the first frame\n",
    "    ret, frame = video.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Could not read the first frame.\")\n",
    "        return\n",
    "\n",
    "    # Set up the initial ROI histogram\n",
    "    roi_frame = frame[y:y+h, x:x+w]\n",
    "    roi_hist = cv2.calcHist([roi_frame], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256])\n",
    "    roi_hist = cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Set the termination criteria for the mean shift\n",
    "    term_criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1)\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Back-project the histogram to get the probability image\n",
    "        dst = cv2.calcBackProject([hsv], [0, 1, 2], roi_hist, [0, 180, 0, 256, 0, 256], 1)\n",
    "\n",
    "        # Apply mean shift to get the new location\n",
    "        ret, roi = cv2.meanShift(dst, roi, term_criteria)\n",
    "\n",
    "        # Draw the tracking result on the frame\n",
    "        x, y, w, h = roi\n",
    "        result_frame = cv2.rectangle(frame.copy(), (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "        # Write the frame to the output video\n",
    "        output_video.write(result_frame)\n",
    "\n",
    "        # Display the result (optional)\n",
    "        cv2.imshow('Tracking Result', result_frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release resources\n",
    "    video.release()\n",
    "    output_video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    print(f\"Object tracking completed. Output video saved to {output_video_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_video_path = \"C:\\\\Users\\\\hp\\\\newRunning.mp4\"\n",
    "    output_video_path =  \"C:\\\\Users\\\\hp\\\\output_histogram_tracking.mp4\"\n",
    "    \n",
    "    # Set the initial ROI coordinates (x, y, width, height)\n",
    "    initial_roi_coords = (100, 100, 50, 50)\n",
    "\n",
    "    histogram_based_tracking(input_video_path, output_video_path, initial_roi_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ec732b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml') \n",
    "video_capture = cv2.VideoCapture(\"C:\\\\Users\\\\hp\\\\opp.mp4\")\n",
    "count = 0\n",
    "while video_capture.isOpened():\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "        if count >= 10:\n",
    "            break\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces= face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(38, 38))\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "\n",
    "        cv2.imwrite(f\"Frame {count}.jpg\",frame)\n",
    "\n",
    "        count +=1\n",
    "        cv2.imshow(f'Detected Face Frame {count}', frame)\n",
    "        print (\"Check you folder to see the images.\")\n",
    "\n",
    "video_capture.release()\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b7a6c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "def save_detected_faces(video_path, output_folder):\n",
    "    # Load the pre-trained face cascade classifier from OpenCV\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Open the video file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Create the output folder if it doesn't exist\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Read a frame from the video\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Break the loop if the video has no more frames\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale for face detection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the frame\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        # Process each detected face\n",
    "        for i, (x, y, w, h) in enumerate(faces):\n",
    "            # Draw a rectangle around the detected face\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # Save the detected face as an image\n",
    "            face_image = frame[y:y + h, x:x + w]\n",
    "            output_path = os.path.join(output_folder, f\"frame_{frame_count}_face_{i}.jpg\")\n",
    "            cv2.imwrite(output_path, face_image)\n",
    "\n",
    "        # Increment frame count\n",
    "        frame_count += 1\n",
    "\n",
    "    # Release resources\n",
    "    cap.release()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    video_path = \"C:\\\\Users\\\\hp\\\\Pictures\\\\channel.mp4\"\n",
    "    output_folder = \"C:\\\\Users\\\\hp\\\\op1.jpg\"\n",
    "\n",
    "    save_detected_faces(video_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd7630",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
