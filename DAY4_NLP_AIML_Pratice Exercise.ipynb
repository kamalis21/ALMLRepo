{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe04e196",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Ths is an exmple of a sentnce wth smlling mistkes.\n",
      "Corrected Sentence: th is an example of a sentence nth selling misken .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download the 'words' and 'stopwords' corpora\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Download the 'punkt' resource for sentence tokenization\n",
    "nltk.download('punkt')\n",
    "\n",
    "def spell_checker(word):\n",
    "    # Case normalization\n",
    "    word = word.lower()\n",
    "\n",
    "    # Skip spell check for stopwords and punctuation\n",
    "    if word in stopwords.words('english') or word in string.punctuation:\n",
    "        return word\n",
    "\n",
    "    # Check if the word is valid\n",
    "    if word in words.words():\n",
    "        return word\n",
    "\n",
    "    # Find the closest word based on edit distance\n",
    "    suggestions = min(words.words(), key=lambda valid_word: edit_distance(word, valid_word))\n",
    "\n",
    "    return suggestions\n",
    "\n",
    "def correct_spelling(sentence):\n",
    "    # Tokenize the sentence\n",
    "    nltk.download('punkt')  # Download 'punkt' here to resolve the issue\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Apply spell check on each word\n",
    "    corrected_tokens = [spell_checker(word) for word in tokens]\n",
    "\n",
    "    # Join the corrected tokens into a sentence\n",
    "    corrected_sentence = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example usage:\n",
    "input_sentence = \"Ths is an exmple of a sentnce wth smlling mistkes.\"\n",
    "output_sentence = correct_spelling(input_sentence)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Corrected Sentence:\", output_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216c6952",
   "metadata": {},
   "source": [
    "# Get a list of valid words in the English language using NLTK's list of words (Hint: use nltk.download('words') to get the raw list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f055292",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Valid Words:\n",
      "['pentathlon', 'thiotungstic', 'threefold', 'overrunner', 'scowbank', 'tamability', 'kildee', 'Desmarestia', 'enchytrae', 'smifligation']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the list of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid words\n",
    "valid_words = set(words.words())\n",
    "\n",
    "# Example: Printing the first 10 valid words\n",
    "print(\"List of Valid Words:\")\n",
    "print(list(valid_words)[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35280e7",
   "metadata": {},
   "source": [
    "# Look at the first 20 words in the list. Is the casing normalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b232d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the list of English words:\n",
      "['A', 'a', 'aa', 'aal', 'aalii', 'aam', 'Aani', 'aardvark', 'aardwolf', 'Aaron', 'Aaronic', 'Aaronical', 'Aaronite', 'Aaronitic', 'Aaru', 'Ab', 'aba', 'Ababdeh', 'Ababua', 'abac']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the list of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid words\n",
    "valid_words = words.words()\n",
    "\n",
    "# Print the first 20 words\n",
    "print(\"First 20 words in the list of English words:\")\n",
    "print(valid_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e3b48a",
   "metadata": {},
   "source": [
    "# 3. Normalize the casing for all the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2386192a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the list of English words with normalized casing:\n",
      "['a', 'a', 'aa', 'aal', 'aalii', 'aam', 'aani', 'aardvark', 'aardwolf', 'aaron', 'aaronic', 'aaronical', 'aaronite', 'aaronitic', 'aaru', 'ab', 'aba', 'ababdeh', 'ababua', 'abac']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the list of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid words and normalize the casing\n",
    "valid_words = [word.lower() for word in words.words()]\n",
    "\n",
    "# Print the first 20 words with normalized casing\n",
    "print(\"First 20 words in the list of English words with normalized casing:\")\n",
    "print(valid_words[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aee41dc",
   "metadata": {},
   "source": [
    "# 4. Some duplicates would have been induced, create unique list after normalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73807c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 20 words in the unique list of English words with normalized casing:\n",
      "['pentathlon', 'thiotungstic', 'threefold', 'overrunner', 'scowbank', 'tamability', 'kildee', 'enchytrae', 'smifligation', 'shaikiyeh', 'hawked', 'miller', 'prisonlike', 'unimpressive', 'cardiophobia', 'musculospiral', 'vermetidae', 'carucated', 'attribution', 'unquietude']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "\n",
    "# Download the list of English words\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid words and normalize the casing\n",
    "normalized_valid_words = set(word.lower() for word in words.words())\n",
    "\n",
    "# Print the first 20 words in the unique list with normalized casing\n",
    "print(\"First 20 words in the unique list of English words with normalized casing:\")\n",
    "print(list(normalized_valid_words)[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68e66a",
   "metadata": {},
   "source": [
    "# 5. Create a list of stop words which should include:\n",
    "\n",
    "i. Stop words from NLTK\n",
    "\n",
    "ii. All punctuations (Hint: use 'punctuation' from string module)\n",
    "\n",
    "Final list should be a combination of these two\n",
    "\n",
    "Define a function to get correct a single term\n",
    "\n",
    "find its edit distance with each term in the valid word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e903ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Term: misteak\n",
      "Corrected Term: misread\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords and words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# Create a list of stop words including NLTK stopwords and punctuations\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Get the list of valid words and normalize the casing\n",
    "normalized_valid_words = set(word.lower() for word in words.words())\n",
    "\n",
    "def correct_term(term):\n",
    "    # Case normalization\n",
    "    term = term.lower()\n",
    "\n",
    "    # Skip correction for stop words\n",
    "    if term in stop_words:\n",
    "        return term\n",
    "\n",
    "    # Check if the term is already a valid word\n",
    "    if term in normalized_valid_words:\n",
    "        return term\n",
    "\n",
    "    # Find the closest word based on edit distance\n",
    "    closest_word = min(normalized_valid_words, key=lambda valid_word: edit_distance(term, valid_word))\n",
    "\n",
    "    return closest_word\n",
    "\n",
    "# Example usage:\n",
    "input_term = \"misteak\"\n",
    "corrected_term = correct_term(input_term)\n",
    "print(\"Input Term:\", input_term)\n",
    "print(\"Corrected Term:\", corrected_term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36439d7d",
   "metadata": {},
   "source": [
    "# Define a function to get correct a single term\n",
    "\n",
    ". For a given term, find its edit distance with each term in the valid word list. To speed up execution, you can use the first 20,000 entries in the\n",
    "\n",
    "valid word list.\n",
    "\n",
    "Store the result in a dictionary, the key as the term, and edit distance as value.\n",
    "\n",
    "Sort the dictionary in ascending order of the values. Return the first entry in the sorted result (value with minimum edit\n",
    "\n",
    "distance). Using the function, get the correct word for committee.\n",
    "\n",
    "Make a set from the list of valid words, for faster lookup to see if word is in valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f4a534d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Term: comittee\n",
      "Corrected Term: admittee\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Download NLTK words\n",
    "nltk.download('words')\n",
    "\n",
    "# Get the list of valid words and normalize the casing for the first 20,000 entries\n",
    "valid_words = set(word.lower() for word in words.words()[:20000])\n",
    "\n",
    "def correct_term(term):\n",
    "    # Case normalization\n",
    "    term = term.lower()\n",
    "\n",
    "    # Check if the term is already a valid word\n",
    "    if term in valid_words:\n",
    "        return term\n",
    "\n",
    "    # Calculate edit distance for the first 20,000 entries in the valid word list\n",
    "    edit_distances = {valid_word: edit_distance(term, valid_word) for valid_word in valid_words}\n",
    "\n",
    "    # Sort the dictionary by edit distance in ascending order\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the corrected term (the first entry in the sorted result)\n",
    "    corrected_term = sorted_distances[0][0]\n",
    "\n",
    "    return corrected_term\n",
    "\n",
    "# Example usage:\n",
    "input_term = \"comittee\"\n",
    "corrected_term = correct_term(input_term)\n",
    "print(\"Input Term:\", input_term)\n",
    "print(\"Corrected Term:\", corrected_term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f270367",
   "metadata": {},
   "source": [
    "# 7. list or not. Define a function for spelling correction in any given input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "455e786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Ths is an exmple of a sentnce wth smlling mistkes.\n",
      "Corrected Sentence: bas is an ample of a bedunce bah ambling alytes .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords and words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# Create a list of stop words including NLTK stopwords and punctuations\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Get the list of valid words and normalize the casing for the first 20,000 entries\n",
    "valid_words = set(word.lower() for word in words.words()[:20000])\n",
    "\n",
    "def correct_term(term):\n",
    "    # Case normalization\n",
    "    term = term.lower()\n",
    "\n",
    "    # Skip correction for stop words\n",
    "    if term in stop_words:\n",
    "        return term\n",
    "\n",
    "    # Check if the term is already a valid word\n",
    "    if term in valid_words:\n",
    "        return term\n",
    "\n",
    "    # Calculate edit distance for the first 20,000 entries in the valid word list\n",
    "    edit_distances = {valid_word: edit_distance(term, valid_word) for valid_word in valid_words}\n",
    "\n",
    "    # Sort the dictionary by edit distance in ascending order\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the corrected term (the first entry in the sorted result)\n",
    "    corrected_term = sorted_distances[0][0]\n",
    "\n",
    "    return corrected_term\n",
    "\n",
    "def correct_spelling(sentence):\n",
    "    # Tokenize the sentence\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "    # Apply spell check on each word\n",
    "    corrected_tokens = [correct_term(word) for word in tokens]\n",
    "\n",
    "    # Join the corrected tokens into a sentence\n",
    "    corrected_sentence = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example usage:\n",
    "input_sentence = \"Ths is an exmple of a sentnce wth smlling mistkes.\"\n",
    "output_sentence = correct_spelling(input_sentence)\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Corrected Sentence:\", output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86bdc760",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. list or not. Define a function for spelling correction in any given input sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7bf17d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Ths is an exmple of a sentnce wth smlling mistkes.\n",
      "Corrected Sentence: bas is an ample of a bedunce bah ambling alytes .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords and words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# Create a list of stop words including NLTK stopwords and punctuations\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Get the list of valid words and normalize the casing for the first 20,000 entries\n",
    "valid_words_set = set(word.lower() for word in words.words()[:20000])\n",
    "\n",
    "def correct_term(term):\n",
    "    # Case normalization\n",
    "    term = term.lower()\n",
    "\n",
    "    # Skip correction for stop words\n",
    "    if term in stop_words:\n",
    "        return term\n",
    "\n",
    "    # Check if the term is already a valid word\n",
    "    if term in valid_words_set:\n",
    "        return term\n",
    "\n",
    "    # Calculate edit distance for the first 20,000 entries in the valid word list\n",
    "    edit_distances = {valid_word: edit_distance(term, valid_word) for valid_word in valid_words_set}\n",
    "\n",
    "    # Sort the dictionary by edit distance in ascending order\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the corrected term (the first entry in the sorted result)\n",
    "    corrected_term = sorted_distances[0][0]\n",
    "\n",
    "    return corrected_term\n",
    "\n",
    "def correct_spelling(sentence):\n",
    "    # Tokenize and normalize the case for each term\n",
    "    tokens = [term.lower() for term in nltk.word_tokenize(sentence)]\n",
    "\n",
    "    # Apply spell check on each term\n",
    "    corrected_tokens = [term if term in valid_words_set else correct_term(term) for term in tokens]\n",
    "\n",
    "    # Join the corrected terms into a sentence\n",
    "    corrected_sentence = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example usage:\n",
    "input_sentence = \"Ths is an exmple of a sentnce wth smlling mistkes.\"\n",
    "output_sentence = correct_spelling(input_sentence)\n",
    "\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Corrected Sentence:\", output_sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a72a58",
   "metadata": {},
   "source": [
    "#8.1. To tokenize them after making all the terms in lowercase\n",
    "\n",
    "For each term in the tokenized sentence: 2. Check if the term is in the list of valid words (valid_words_set).\n",
    "\n",
    "3. If yes, return the word as is.\n",
    "\n",
    "4. If no, get the correct word using get_correct_term function.\n",
    "\n",
    "5. To return the joined string as output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dbf8a3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\hp\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: Ths is an exmple of a sentnce wth smlling mistkes.\n",
      "Corrected Sentence: bas is an ample of a bedunce bah ambling alytes .\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import words\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Download NLTK stopwords and words\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "\n",
    "# Create a list of stop words including NLTK stopwords and punctuations\n",
    "stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "\n",
    "# Get the list of valid words and normalize the casing for the first 20,000 entries\n",
    "valid_words_set = set(word.lower() for word in words.words()[:20000])\n",
    "\n",
    "def correct_term(term):\n",
    "    # Case normalization\n",
    "    term = term.lower()\n",
    "\n",
    "    # Skip correction for stop words\n",
    "    if term in stop_words:\n",
    "        return term\n",
    "\n",
    "    # Check if the term is already a valid word\n",
    "    if term in valid_words_set:\n",
    "        return term\n",
    "\n",
    "    # Calculate edit distance for the first 20,000 entries in the valid word list\n",
    "    edit_distances = {valid_word: edit_distance(term, valid_word) for valid_word in valid_words_set}\n",
    "\n",
    "    # Sort the dictionary by edit distance in ascending order\n",
    "    sorted_distances = sorted(edit_distances.items(), key=lambda x: x[1])\n",
    "\n",
    "    # Return the corrected term (the first entry in the sorted result)\n",
    "    corrected_term = sorted_distances[0][0]\n",
    "\n",
    "    return corrected_term\n",
    "\n",
    "def correct_spelling(sentence):\n",
    "    # Tokenize and normalize the case for each term\n",
    "    tokens = [term.lower() for term in nltk.word_tokenize(sentence)]\n",
    "\n",
    "    # Apply spell check on each term\n",
    "    corrected_tokens = [term if term in valid_words_set else correct_term(term) for term in tokens]\n",
    "\n",
    "    # Join the corrected terms into a sentence\n",
    "    corrected_sentence = ' '.join(corrected_tokens)\n",
    "\n",
    "    return corrected_sentence\n",
    "\n",
    "# Example usage:\n",
    "input_sentence = \"Ths is an exmple of a sentnce wth smlling mistkes.\"\n",
    "output_sentence = correct_spelling(input_sentence)\n",
    "\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Corrected Sentence:\", output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f704f",
   "metadata": {},
   "outputs": [],
   "source": [
    "9Test the function for the input sentence “The new abacos is great”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eabafdef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence: The new abacos is great\n",
      "Corrected Sentence: the anew abacus is arear\n"
     ]
    }
   ],
   "source": [
    "input_sentence = \"The new abacos is great\"\n",
    "output_sentence = correct_spelling(input_sentence)\n",
    "\n",
    "print(\"Input Sentence:\", input_sentence)\n",
    "print(\"Corrected Sentence:\", output_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99544807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
